---
title: "gestational_diabetes"
output: pdf_document
date: "2023-05-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("caret")
```

#Library
```{r}
library(ggplot2)
library(dplyr)
library(gridExtra)
library(naniar)
library(visdat)
library(corrplot)
library(caret)
```


#Data
```{r}
data <- read.csv(url("https://raw.githubusercontent.com/Marchosh/gestational_diabetes/master/patients.csv?token=GHSAT0AAAAAACC7K4VFNJCC5HTZ23CRGRBAZDMELKA"))
head(data)
```


```{r}
summary(data)

```
Pregnancies:
The range of the number of pregnancies is from 0 to 17, with a median of 3 and a mean of approximately 3.845.
The majority of individuals (75%) had 6 or fewer pregnancies (as indicated by the third quartile).

Glucose:
The minimum glucose level observed is 0, which seems unusual and may indicate missing or invalid data.
The range of glucose levels is from 0 to 199, with a median of 117 and a mean of approximately 120.9.
The majority of glucose levels (75%) fall below 140.2 (as indicated by the third quartile).

BloodPressure:
The range of diastolic blood pressure values is from 0 to 122, with a median of 72 and a mean of approximately 69.11.
The majority of blood pressure readings (75%) are below 80 (as indicated by the third quartile).

SkinThickness:
The range of triceps skinfold thickness values is from 0 to 99, with a median of 23 and a mean of approximately 20.54.
The majority of skinfold thickness measurements (75%) are below 32 (as indicated by the third quartile).

Insulin:
The range of serum insulin levels is from 0 to 846, with a median of 30.5 and a mean of approximately 79.8.
The majority of insulin levels (75%) are below 127.2 (as indicated by the third quartile).

BMI:
The range of body mass index (BMI) values is from 0 to 67.1, with a median of 32 and a mean of approximately 31.99.
The majority of individuals (75%) have a BMI below 36.6 (as indicated by the third quartile).

Pedigree:
The range of diabetes pedigree function values is from 0.0780 to 2.4200, with a median of 0.3725 and a mean of approximately 0.4719.
The majority of pedigree function values (75%) fall below 0.6262 (as indicated by the third quartile).

Age:
The range of ages in the dataset is from 21 to 81, with a median of 29 and a mean of approximately 33.24.
The majority of individuals (75%) are below 41 years old (as indicated by the third quartile).

Diagnosis:
The diagnosis variable is binary, with 0 representing a negative diagnosis (no diabetes) and 1 representing a positive diagnosis (diabetes).
Approximately 34.9% of the cases in the dataset are diagnosed with diabetes, as indicated by the mean.

# EDA
```{r}
# Bar plot of Diagnosis
ggplot(data, aes(x = factor(Diagnosis))) +
  geom_bar(fill = "lightblue") +
  labs(x = "Diagnosis", y = "Count") +
  ggtitle("Distribution of Diagnosis")

```

```{r}
# Exclude non-numeric variables
vars <- names(data)[sapply(data, is.numeric)]

# Loop through the variables and create scatter plots
for (var in vars) {
  plot <- ggplot(data, aes(x = .data[[var]], y = Diagnosis, color = as.factor(Diagnosis))) +
    geom_point() +
    labs(x = var, y = "Diagnosis", color = "Diagnosis") +
    ggtitle(paste("Scatter Plot of", var, "against Diagnosis")) +
    theme(legend.position = "bottom") +
    scale_color_brewer(palette = "Set1")
  
  print(plot)
}

```
```{r}
# Exclude non-numeric variables and the "Diagnosis" column
vars <- names(data)[!sapply(data, is.factor)]
vars <- vars[vars != "Diagnosis"]

# Create a list to store the plots
plots <- list()

# Loop through the variables and create box plots
for (var in vars) {
  plot <- ggplot(data, aes(x = as.factor(Diagnosis), y = .data[[var]], fill = as.factor(Diagnosis))) +
    geom_boxplot() +
    labs(x = "Diagnosis", y = var, fill = "Diagnosis") +
    ggtitle(paste("Box Plot of", var, "by Diagnosis")) +
    theme(legend.position = "bottom") +
    scale_fill_brewer(palette = "Set1")
  
  plots[[var]] <- plot
}

# Display the plots
for (plot in plots) {
  print(plot)
}

```
## histogram
```{r}
# Create histograms for each variable
numeric_vars <- names(data)[sapply(data, is.numeric)]
for (var in numeric_vars) {
  p <- ggplot(data, aes(x = .data[[var]])) +
    geom_histogram(binwidth = 5, fill = "lightblue", color = "black") +
    labs(x = var, y = "Frequency", title = paste("Histogram of", var)) +
    theme_minimal()
  
  print(p)
}
```
Pregnancies:

Glucose:
there is zero value which missing value mostlikely

Blood Preasue:
there si possibility of missing value in the blood preaseure since 0 is present

Skin thickness:
missing value 0

Insulin:
hug spike in 0 must be investivigated

BMI:
0 porbabluy missing value

Pedigree:
all equal, need to be investivigated

AGE:
seem some outlier with women over 60

Diagnosis:
Binary

## violin plot
```{r}
# Create violin plots for each variable
numeric_vars <- names(data)[sapply(data, is.numeric)]
for (var in numeric_vars) {
  p <- ggplot(data, aes(x = as.factor(Diagnosis), y = .data[[var]], fill = as.factor(Diagnosis))) +
    geom_violin(trim = FALSE, scale = "width") +
    labs(x = "Diagnosis", y = var, title = paste("Violin Plot of", var, "by Diagnosis")) +
    theme_minimal()
  
  print(p)
}

```

```{r}
# Create density plots for each variable
numeric_vars <- names(data)[sapply(data, is.numeric)]
for (var in numeric_vars) {
  p <- ggplot(data, aes(x = .data[[var]], fill = as.factor(Diagnosis))) +
    geom_density(alpha = 0.5) +
    labs(x = var, y = "Density", title = paste("Density Plot of", var, "by Diagnosis")) +
    theme_minimal()
  print(p)
}

```


# missing value
```{r}
# Calculate the missing values
missing_data  <- colSums(is.na(data))

# Create a bar plot of missing values
barplot(missing_data , xlab = "Variables", ylab = "Missing Values", main = "Missing Values by Variable")

```


```{r}
# Calculate the number of missing values for each variable
missing_counts <- data %>%
  summarise_all(~sum(is.na(.) | . == 0))

# Convert missing_counts to long format for plotting
missing_counts_long <- tidyr::pivot_longer(missing_counts, everything(), names_to = "Variable", values_to = "Missing_Count")

# Create the bar plot
p <- ggplot(missing_counts_long, aes(x = Variable, y = Missing_Count, fill = Variable)) +
  geom_bar(stat = "identity") +
  labs(x = "Variable", y = "Missing Count", title = "Number of Missing Values by Variable") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p)

```

```{r}
# Create a new dataframe called missdata
missdata <- data

# Replace 0 with NA for missing values in missdata
missdata[missdata == 0] <- NA

# Plot the modified missing values heatmap
vis_miss(missdata)
```


Diagnosis 0 is category so is not mising value,
THe insulin 0 is need to be investivigate wheter is it true 0 or missing value beacsue 49% of them is 0
Same goes fro skin thcikness, wheter is it true 0 or missing value becaseu contain 30%
For the preganancies 0 it could indicate they never pregnant and not missing value
But the other (Glucose, BloodPreassure, and BMi) variable 0 is mostlikely missing value.

We can see straight long line continous from one oclumn to other it showed that,
when 1 of the column missing the other column are also likely to be missing

## Replace missing value (0) in (Glucose, BloodPreassure, and BMi)
```{r}
cleandata <- data

# Calculate the median values
median_glucose <- median(cleandata$Glucose, na.rm = TRUE)
median_bp <- median(cleandata$BloodPressure, na.rm = TRUE)
median_bmi <- median(cleandata$BMI, na.rm = TRUE)

# Replace 0 with median in Glucose column
cleandata$Glucose <- ifelse(cleandata$Glucose == 0, median_glucose, cleandata$Glucose)

# Replace 0 with median in BloodPressure column
cleandata$BloodPressure <- ifelse(cleandata$BloodPressure == 0, median_bp, cleandata$BloodPressure)

# Replace 0 with median in BMI column
cleandata$BMI <- ifelse(cleandata$BMI == 0, median_bmi, cleandata$BMI)
```


```{r}
# Calculate the number of missing values for each variable
missing_counts <- cleandata %>%
  summarise_all(~sum(is.na(.) | . == 0))

# Convert missing_counts to long format for plotting
missing_counts_long <- tidyr::pivot_longer(missing_counts, everything(), names_to = "Variable", values_to = "Missing_Count")

# Create the bar plot
p <- ggplot(missing_counts_long, aes(x = Variable, y = Missing_Count, fill = Variable)) +
  geom_bar(stat = "identity") +
  labs(x = "Variable", y = "Missing Count", title = "Number of Missing Values by Variable") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p)

```

```{r}
head(cleandata)
```



# Outlier
```{r}

# Create box plot for each variable
for (var in names(cleandata)) {
  p <- ggplot(cleandata, aes(x = 1, y = cleandata[[var]])) +
    geom_boxplot(fill = "lightblue", color = "black") +
    labs(x = "", y = var, title = paste("Box Plot of", var)) +
    theme_minimal() +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
  print(p)
}
```


```{r}
# Assign independent variables
independent_vars <- names(cleandata)[names(cleandata) != "Diagnosis"]

# Calculate the IQR for each independent variable
iqr_values <- apply(cleandata[, independent_vars], 2, IQR)

# Find the lower and upper bounds for outliers
lower_bounds <- apply(cleandata[, independent_vars], 2, function(x) quantile(x, 0.25) - 1.5 * IQR(x))
upper_bounds <- apply(cleandata[, independent_vars], 2, function(x) quantile(x, 0.75) + 1.5 * IQR(x))

# Identify the outliers for each variable
outliers <- lapply(seq_along(independent_vars), function(i) {
  outliers <- which(cleandata[, independent_vars[i]] < lower_bounds[i] | cleandata[, independent_vars[i]] > upper_bounds[i])
  if (length(outliers) > 0) {
    data.frame(Variable = independent_vars[i], Outlier_Value = cleandata[outliers, independent_vars[i]])
  } else {
    NULL
  }
})

# Combine the outliers into a single data frame
outliers_df <- do.call(rbind, outliers)

outliers_df
```


## Replace Outlier

Robust Statistical Methods: Robust statistical methods, such as robust regression or robust estimators, can handle outliers more effectively by downweighting their influence in the analysis. These methods are less sensitive to extreme values and can provide more reliable estimates.

Winsorization: Winsorization replaces extreme values with values closer to the rest of the data. Instead of removing the outliers completely, you can replace them with a trimmed or truncated value at a certain percentile. This approach retains the overall distribution shape while reducing the impact of outliers

```{r}
# Apply Winsorization to replace outliers in selected variables
selected_vars <- c("Pregnancies", "BloodPressure", "BMI", "Insulin", "SkinThickness", "Pedigree")

winsorize <- function(x, trim = 0.05) {
  q <- quantile(x, probs = c(trim, 1 - trim), na.rm = TRUE)
  x[x < q[1]] <- q[1]
  x[x > q[2]] <- q[2]
  x
}

for (var in selected_vars) {
  cleandata[, var] <- winsorize(cleandata[, var])
}
```


### Box plot After replacement

```{r}
# Create box plot for each variable
for (var in names(cleandata)) {
  p <- ggplot(cleandata, aes(x = 1, y = cleandata[[var]])) +
    geom_boxplot(fill = "lightblue", color = "black") +
    labs(x = "", y = var, title = paste("Box Plot of", var)) +
    theme_minimal() +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
  print(p)
}

```



# Select Varaible
```{r}
# Calculate the correlation matrix
cor_matrix <- cor(cleandata)

# Create a correlogram using a heatmap

corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7)


```

Glucose has the strongest correlation with the Diagnosis

There is also posisbilities of multicoloniarity between Preagnancies and Age
and between Insulin and skinthcikness

# Standarization
Standardizing features to a Gaussian distribution can be a good idea for several reasons:

Equalize the scales: Standardizing the features ensures that they are on a comparable scale. This is important when working with algorithms that are sensitive to the scale of the variables. Standardization prevents features with larger scales from dominating the algorithm and helps to ensure fair comparisons between different features.

Facilitate model convergence: Many machine learning algorithms, such as linear regression and neural networks, rely on optimization techniques that assume the input features are normally distributed or have a similar scale. Standardizing the features can improve the convergence of these algorithms and help them find the optimal solution more efficiently.

Interpretability and comparability: When features are standardized, their values are transformed to a common scale, making them more interpretable and comparable. The standardized values represent the number of standard deviations the original values are from the mean. This allows for easier interpretation and understanding of the relative importance and impact of each feature on the model.

Reduce the influence of outliers: Standardization can help mitigate the impact of outliers on the model. By transforming the features to a Gaussian distribution, extreme values (outliers) are scaled to a smaller range and have less influence on the model's behavior. This can lead to more robust and stable model performance.

Improve feature importance estimation: Standardization helps to provide a fair estimation of feature importance or feature contribution in models that use regularization techniques or rely on feature weights. It ensures that the importance or weights are not biased by the original scale of the features.

Overall, standardizing features to a Gaussian distribution is a common practice in data preprocessing to improve the performance, stability, and interpretability of machine learning models. It helps to address issues related to feature scales, convergence, interpretability, outliers, and fair comparison between features.


```{r}
standdata <- cleandata

# Select the independent variables
independent_vars <- names(cleandata)[names(cleandata) != "Diagnosis"]

# Standardize the independent variables using Gaussian distribution
stand_data <- as.data.frame(scale(cleandata[, independent_vars]))

standdata[, independent_vars] <- stand_data
head(standdata, 10)
```
```{r}
summary(standdata)

```


```{r}
# Load the required library
library(glmnet)

# Create the formula for logistic regression
formula <- as.formula("Diagnosis ~ .")

# Fit the logistic regression model
LRM1 <- glm(formula, data = standdata, family = "binomial")

summary(LRM1)
```

```{r}
# Predict using the model
predictions <- predict(LRM1, newdata = standdata, type = "response")

# Convert predicted probabilities to class labels (0 or 1)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# Convert predicted_classes and standdata$Diagnosis to factors with the same levels
predicted_classes <- factor(predicted_classes, levels = c(0, 1))
standdata$Diagnosis <- factor(standdata$Diagnosis, levels = c(0, 1))

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predicted_classes, standdata$Diagnosis)

# Calculate accuracy
accuracy <- confusion_matrix$overall['Accuracy']

# Print the accuracy
print(accuracy)

```




# Anova
```{r}
# Fit a logistic regression model
LRM1 <- glm(Diagnosis ~ ., data = standdata, family = "binomial")

# Obtain the p-values for each predictor variable
p_values <- summary(LRM1)$coefficients[-1, "Pr(>|z|)"]

# Adjust the p-values for multiple comparisons
adjusted_p_values <- p.adjust(p_values, method = "holm")

# Create a data frame with variable names and adjusted p-values
anova_results <- data.frame(variable = colnames(standdata)[-ncol(standdata)],
                            p_value = adjusted_p_values)

# Filter significant predictors based on a significance threshold (e.g., 0.05)
significant_predictors <- subset(anova_results, p_value < 0.05)

# Print the significant predictors
print(significant_predictors)

```


```{r}
# Fit a logistic regression model with selected predictors
LRM2 <- glm(Diagnosis ~ Pregnancies + Glucose + BMI + Pedigree, data = standdata, family = "binomial")

# Print the summary of the model
summary(LRM2)


```
```{r}
# Predict using the model
predicted_classes <- ifelse(predict(LRM2, newdata = standdata, type = "response") > 0.5, 1, 0)


# Create a confusion matrix
confusion_matrix <- table(predicted_classes, standdata$Diagnosis)

# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

# Print the confusion matrix and accuracy
print(confusion_matrix)
print(paste("Accuracy:", accuracy))

```


