---
title: "gestational_diabetes"
output: pdf_document
date: "2023-05-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
install.packages("visdat")
```

#Library
```{r}
library(ggplot2)
library(dplyr)
library(gridExtra)
library(naniar)
library(visdat)
library(corrplot)
library(caret)
library(ROSE)
```


# a) Data
```{r}
data <- read.csv(url("https://raw.githubusercontent.com/Marchosh/gestational_diabetes/master/patients.csv?token=GHSAT0AAAAAACC7K4VFNJCC5HTZ23CRGRBAZDMELKA"))
head(data)
```

There are a lot of missing values and other values that seem like they are outliers. They are either way to high or way to low

# B) Summary statistics
```{r}
summary(data)

```
Pregnancies:
The range of the number of pregnancies is from 0 to 17, with a median of 3 and a mean of approximately 3.845.
The majority of individuals (75%) had 6 or fewer pregnancies (as indicated by the third quartile).

Glucose:
The minimum glucose level observed is 0, which seems unusual and may indicate missing or invalid data.
The range of glucose levels is from 0 to 199, with a median of 117 and a mean of approximately 120.9.
The majority of glucose levels (75%) fall below 140.2 (as indicated by the third quartile).

BloodPressure:
The range of diastolic blood pressure values is from 0 to 122, with a median of 72 and a mean of approximately 69.11.
The majority of blood pressure readings (75%) are below 80 (as indicated by the third quartile).

SkinThickness:
The range of triceps skinfold thickness values is from 0 to 99, with a median of 23 and a mean of approximately 20.54.
The majority of skinfold thickness measurements (75%) are below 32 (as indicated by the third quartile).

Insulin:
The range of serum insulin levels is from 0 to 846, with a median of 30.5 and a mean of approximately 79.8.
The majority of insulin levels (75%) are below 127.2 (as indicated by the third quartile).

BMI:
The range of body mass index (BMI) values is from 0 to 67.1, with a median of 32 and a mean of approximately 31.99.
The majority of individuals (75%) have a BMI below 36.6 (as indicated by the third quartile).

Pedigree:
The range of diabetes pedigree function values is from 0.0780 to 2.4200, with a median of 0.3725 and a mean of approximately 0.4719.
The majority of pedigree function values (75%) fall below 0.6262 (as indicated by the third quartile).

Age:
The range of ages in the dataset is from 21 to 81, with a median of 29 and a mean of approximately 33.24.
The majority of individuals (75%) are below 41 years old (as indicated by the third quartile).

Diagnosis:
The diagnosis variable is binary, with 0 representing a negative diagnosis (no diabetes) and 1 representing a positive diagnosis (diabetes).
Approximately 34.9% of the cases in the dataset are diagnosed with diabetes, as indicated by the mean.

# EDA
```{r}
# Bar plot of Diagnosis
ggplot(data, aes(x = factor(Diagnosis))) +
  geom_bar(fill = "lightblue") +
  labs(x = "Diagnosis", y = "Count") +
  ggtitle("Distribution of Diagnosis")

```

there is inbalance of the Diagnosis data which we might need to balanced it since it is the target vraible

```{r}
# Exclude non-numeric variables
vars <- names(data)[sapply(data, is.numeric)]

# Loop through the variables and create scatter plots
for (var in vars) {
  plot <- ggplot(data, aes(x = .data[[var]], y = Diagnosis, color = as.factor(Diagnosis))) +
    geom_point() +
    labs(x = var, y = "Diagnosis", color = "Diagnosis") +
    ggtitle(paste("Scatter Plot of", var, "against Diagnosis")) +
    theme(legend.position = "bottom") +
    scale_color_brewer(palette = "Set1")
  
  print(plot)
}

```

It's quite hard to see the relationship of each vraible to Diagnosis, there is many overlapping area, and also some of the outliar are shown

```{r}
# Exclude non-numeric variables and the "Diagnosis" column
vars <- names(data)[!sapply(data, is.factor)]
vars <- vars[vars != "Diagnosis"]

# Create a list to store the plots
plots <- list()

# Loop through the variables and create box plots
for (var in vars) {
  plot <- ggplot(data, aes(x = as.factor(Diagnosis), y = .data[[var]], fill = as.factor(Diagnosis))) +
    geom_boxplot() +
    labs(x = "Diagnosis", y = var, fill = "Diagnosis") +
    ggtitle(paste("Box Plot of", var, "by Diagnosis")) +
    theme(legend.position = "bottom") +
    scale_fill_brewer(palette = "Set1")
  
  plots[[var]] <- plot
}

# Display the plots
for (plot in plots) {
  print(plot)
}

```

```{r}
  #Boxplot of BMI by Diagnosis
ggplot(data, aes(x = Diagnosis, y = BMI, group = Diagnosis)) + geom_boxplot(fill = "lightgreen") + labs(x = "Diagnosis", y = "BMI") + ggtitle("BMI by Diagnosis")

  #Scatterplot of BloodPressure and Age
ggplot(data, aes(x = Age, y = BloodPressure)) + geom_point(color = "blue", alpha = 0.6) + labs(x = "Age", y = "Blood Pressure") + ggtitle("Blood Pressure by Age")

  #Barplot of Pregnancies
ggplot(data, aes(x = Pregnancies)) + geom_bar(fill = "lightpink") + labs(x = "Pregnancies", y = "Count") +  ggtitle("Distribution of Pregnancies")

  #Density Plot of Insulin by Diagnosis
ggplot(data, aes(x = Insulin, fill = Diagnosis)) + geom_density(alpha = 0.5) + labs(x = "Insulin", y = "Density") + ggtitle("Insulin Distribution by Diagnosis")
```

The box plot of BMI by Diagnosis shows the two values of diagnosis (0 and 1) and see how BMI reacts to those values of Diagnosis. 0 represents people who are not diagnosed with diabetes and 1 represents people who have been diagnosed with diabetes. People who are diagnosed with diabetes have a higher BMI while people who are not diagnosed have a lower BMI. The median of people diagnosed is around 35, have more outliers, the highest value of the box is 39, and the lowest is 31. When we look at the people who are not diagnosed with diabetes the median is 30, fewer outliers, the highest value of the box being 36, and the lowest is 26.

The scatter plot of Blood Pressure by Age shows that as age increases blood pressure also increases.

The bar chart of Pregnancies show how many times people have been pregnant. Most people have been pregnant 1 time and the outlier being 17 which is the highest. We can also see that the plot is skewed to the right.

Density graph of Insulin by Diagnosis

## histogram
```{r}
# Create histograms for each variable
numeric_vars <- names(data)[sapply(data, is.numeric)]
for (var in numeric_vars) {
  p <- ggplot(data, aes(x = .data[[var]])) +
    geom_histogram(binwidth = 5, fill = "lightblue", color = "black") +
    labs(x = var, y = "Frequency", title = paste("Histogram of", var)) +
    theme_minimal()
  
  print(p)
}
```
Pregnancies:

Glucose:
The histogram of Glucose shows that most people have a glucose level between 100 and 125. The highest level being 117 and the amount of people with that amount of Glucose level are around the 110.
there is zero value which missing value mostlikely

Blood Preasue:
there si possibility of missing value in the blood preaseure since 0 is present

Skin thickness:
missing value 0

Insulin:
hug spike in 0 must be investivigated

BMI:
0 porbabluy missing value

Pedigree:
all equal, need to be investivigated

AGE:
seem some outlier with women over 60

Diagnosis:
Binary

## violin plot
```{r}
# Create violin plots for each variable
numeric_vars <- names(data)[sapply(data, is.numeric)]
for (var in numeric_vars) {
  p <- ggplot(data, aes(x = as.factor(Diagnosis), y = .data[[var]], fill = as.factor(Diagnosis))) +
    geom_violin(trim = FALSE, scale = "width") +
    labs(x = "Diagnosis", y = var, title = paste("Violin Plot of", var, "by Diagnosis")) +
    theme_minimal()
  
  print(p)
}

```



```{r}
# Create density plots for each variable
numeric_vars <- names(data)[sapply(data, is.numeric)]
for (var in numeric_vars) {
  p <- ggplot(data, aes(x = .data[[var]], fill = as.factor(Diagnosis))) +
    geom_density(alpha = 0.5) +
    labs(x = var, y = "Density", title = paste("Density Plot of", var, "by Diagnosis")) +
    theme_minimal()
  print(p)
}

```

## Corelation Pregnancy Diagnostic
```{r}
pregnancy_counts <-  table(data$Diagnosis, data$Pregnancies)

# Create a side-by-side bar plot
barplot(diagnostic_counts, 
        main = "Number of Pregnancies vs Diagnostic Outcome",
        xlab = "Diagnostic Outcome",
        ylab = "Number of Pregnancies",
        col = c("blue", "red"),
        legend = rownames(pregnancy_counts),
        beside = TRUE)
```

The more number of pregnancies shows higher chance of the person diagnos as GDM

## SKinthckiness vs BMI (diagnsois)
```{r}
# Create a scatter plot with different colors for each diagnosis category
ggplot(data, aes(x = SkinThickness, y = BMI, color = Diagnosis)) +
  geom_point() +
  labs(x = "Skin Thickness", y = "BMI", color = "Diagnosis") +
  theme_minimal()

```

skinThickness and BMi shows a postive corelation but, some of the skin thcikness that has value 0 make the correlation lower.
the skin thickness with value 0 might be potential missing value but consdering the amount so many might influence the original data

# D) missing value

```{r}
# Calculate the number of missing values for each variable
missing_counts <- data %>%
  summarise_all(~sum(is.na(.) | . == 0))

# Convert missing_counts to long format for plotting
missing_counts_long <- tidyr::pivot_longer(missing_counts, everything(), names_to = "Variable", values_to = "Missing_Count")

# Create the bar plot
p <- ggplot(missing_counts_long, aes(x = Variable, y = Missing_Count, fill = Variable)) +
  geom_bar(stat = "identity") +
  labs(x = "Variable", y = "Missing Count", title = "Number of Missing Values by Variable") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p)

```
It shows that Diagnosis has highes since it contatin 0 but we know that it's not missing value but actual value
For the varaible Insulin and skinthckiness and preganncies showing execptionaly high number of 0 which might not a missing value



```{r}
# Create a new dataframe called missdata
missdata <- data

# Replace 0 with NA for missing values in missdata
missdata[missdata == 0] <- NA

# Plot the modified missing values heatmap
vis_miss(missdata)
```

Diagnosis 0 is category so is not mising value,
THe insulin 0 is need to be investivigate wheter is it true 0 or missing value beacsue 49% of them is 0
Same goes fro skin thcikness, wheter is it true 0 or missing value becaseu contain 30%
For the preganancies 0 it could indicate they never pregnant and not missing value
But the other (Glucose, BloodPreassure, and BMi) variable 0 is mostlikely missing value.

We can see straight long line continous from one oclumn to other it showed that,
when 1 of the column missing the other column are also likely to be missing

## Replace missing value (0) in (Glucose, BloodPreassure, and BMi)
```{r}
cleandata <- data

# Calculate the median values
median_glucose <- median(cleandata$Glucose, na.rm = TRUE)
median_bp <- median(cleandata$BloodPressure, na.rm = TRUE)
median_bmi <- median(cleandata$BMI, na.rm = TRUE)

# Replace 0 with median in Glucose column
cleandata$Glucose <- ifelse(cleandata$Glucose == 0, median_glucose, cleandata$Glucose)

# Replace 0 with median in BloodPressure column
cleandata$BloodPressure <- ifelse(cleandata$BloodPressure == 0, median_bp, cleandata$BloodPressure)

# Replace 0 with median in BMI column
cleandata$BMI <- ifelse(cleandata$BMI == 0, median_bmi, cleandata$BMI)
```

## After Cleaning
```{r}
# Calculate the number of missing values for each variable
missing_counts <- cleandata %>%
  summarise_all(~sum(is.na(.) | . == 0))

# Convert missing_counts to long format for plotting
missing_counts_long <- tidyr::pivot_longer(missing_counts, everything(), names_to = "Variable", values_to = "Missing_Count")

# Create the bar plot
p <- ggplot(missing_counts_long, aes(x = Variable, y = Missing_Count, fill = Variable)) +
  geom_bar(stat = "identity") +
  labs(x = "Variable", y = "Missing Count", title = "Number of Missing Values by Variable") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p)

```

# E) Outlier
```{r}
# Create box plot for each variable
for (var in names(cleandata)) {
  p <- ggplot(cleandata, aes(x = 1, y = cleandata[[var]])) +
    geom_boxplot(fill = "lightblue", color = "black") +
    labs(x = "", y = var, title = paste("Box Plot of", var)) +
    theme_minimal() +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
  print(p)
}
```


```{r}
# Assign independent variables
independent_vars <- names(cleandata)[names(cleandata) != "Diagnosis"]

# Calculate the IQR for each independent variable
iqr_values <- apply(cleandata[, independent_vars], 2, IQR)

# Find the lower and upper bounds for outliers
lower_bounds <- apply(cleandata[, independent_vars], 2, function(x) quantile(x, 0.25) - 1.5 * IQR(x))
upper_bounds <- apply(cleandata[, independent_vars], 2, function(x) quantile(x, 0.75) + 1.5 * IQR(x))

# Identify the outliers for each variable
outliers <- lapply(seq_along(independent_vars), function(i) {
  outliers <- which(cleandata[, independent_vars[i]] < lower_bounds[i] | cleandata[, independent_vars[i]] > upper_bounds[i])
  if (length(outliers) > 0) {
    data.frame(Variable = independent_vars[i], Outlier_Value = cleandata[outliers, independent_vars[i]])
  } else {
    NULL
  }
})

# Combine the outliers into a single data frame
outliers_df <- do.call(rbind, outliers)

head(outliers_df)
```


## Replace Outlier
Winsorization: Winsorization replaces extreme values with values closer to the rest of the data. Instead of removing the outliers completely, you can replace them with a trimmed or truncated value at a certain percentile. This approach retains the overall distribution shape while reducing the impact of outliers

We used winsorization as our method of replacing outliers. First we identified the desired percentile values which are the 5th and 95th percentiles. These percentiles define the range within which the values will be capped. The next step was to determine the cutoff values. The 5th percentile represents the lower cutoff and the 95th percentile represents the upper cutoff. Any value below the lower cutoff will be replaced with the value of the lower cutoff and any value above the upper cutoff will be replaced with the upper cutoff. 


```{r}
# Apply Winsorization to replace outliers in selected variables
selected_vars <- c("Pregnancies", "BloodPressure", "BMI", "Insulin", "SkinThickness", "Pedigree")

winsorize <- function(x, trim = 0.05) {
  q <- quantile(x, probs = c(trim, 1 - trim), na.rm = TRUE)
  x[x < q[1]] <- q[1]
  x[x > q[2]] <- q[2]
  x
}

for (var in selected_vars) {
  cleandata[, var] <- winsorize(cleandata[, var])
}
```


### Box plot After replacement

```{r}
# Create box plot for each variable
for (var in names(cleandata)) {
  p <- ggplot(cleandata, aes(x = 1, y = cleandata[[var]])) +
    geom_boxplot(fill = "lightblue", color = "black") +
    labs(x = "", y = var, title = paste("Box Plot of", var)) +
    theme_minimal() +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
  print(p)
}

```



# G) Select Varaible from correlation table

## pairs plot
```{r}
library(psych)

pairs.panels(cleandata)
```


```{r}
# Calculate the correlation matrix
cor_matrix <- cor(cleandata)

# Create a correlogram using a heatmap

corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7, addCoef.col = "black")


```

Glucose has the strongest correlation with the Diagnosis

There is also posisbilities of multicoloniarity between Preagnancies and Age
and between Insulin and skinthcikness

Skinthickness and Insulin is removed since it has great umber of 0


# H) Standarization
Standardizing features to a Gaussian distribution can be a good idea for several reasons:

Equalize the scales: Standardizing the features ensures that they are on a comparable scale. This is important when working with algorithms that are sensitive to the scale of the variables. Standardization prevents features with larger scales from dominating the algorithm and helps to ensure fair comparisons between different features.

Facilitate model convergence: Many machine learning algorithms, such as linear regression and neural networks, rely on optimization techniques that assume the input features are normally distributed or have a similar scale. Standardizing the features can improve the convergence of these algorithms and help them find the optimal solution more efficiently.

Interpretability and comparability: When features are standardized, their values are transformed to a common scale, making them more interpretable and comparable. The standardized values represent the number of standard deviations the original values are from the mean. This allows for easier interpretation and understanding of the relative importance and impact of each feature on the model.

Reduce the influence of outliers: Standardization can help mitigate the impact of outliers on the model. By transforming the features to a Gaussian distribution, extreme values (outliers) are scaled to a smaller range and have less influence on the model's behavior. This can lead to more robust and stable model performance.

Improve feature importance estimation: Standardization helps to provide a fair estimation of feature importance or feature contribution in models that use regularization techniques or rely on feature weights. It ensures that the importance or weights are not biased by the original scale of the features.

Overall, standardizing features to a Gaussian distribution is a common practice in data preprocessing to improve the performance, stability, and interpretability of machine learning models. It helps to address issues related to feature scales, convergence, interpretability, outliers, and fair comparison between features.


```{r}
standdata <- cleandata

# Select the independent variables
independent_vars <- names(cleandata)[names(cleandata) != "Diagnosis"]

# Standardize the independent variables using Gaussian distribution
stand_data <- as.data.frame(scale(cleandata[, independent_vars]))

standdata[, independent_vars] <- stand_data
head(standdata, 10)
```
```{r}
summary(standdata)

```


It is good to standardize the feature so we can compare the variables, we can avoid bias, and we can assume that we have a normal distibution.

# Normalize Inbalance Data (Diagnosis)
```{r}
# Convert "Diagnosis" column to factor
standdata$Diagnosis <- as.factor(standdata$Diagnosis)

# Check class distribution
table(standdata$Diagnosis)

# Get the indices of the minority and majority class
minority_indices <- which(standdata$Diagnosis == 1)
majority_indices <- which(standdata$Diagnosis == 0)

# Oversample the minority class
oversampled_minority_indices <- sample(minority_indices, 500, replace = TRUE)

# Undersample the majority class
undersampled_majority_indices <- sample(majority_indices, 500)

# Combine the minority and balanced majority indices
balanced_indices <- c(oversampled_minority_indices, undersampled_majority_indices)

# Create the balanced dataset
balanced_data <- standdata[balanced_indices, ]

# Check the class distribution of the balanced dataset
table(balanced_data$Diagnosis)
```

# I) Model LRM1 (logistic regression with slected varaibles)

```{r}
# Load the required library
library(glmnet)

# Create the formula for logistic regression
formula <- as.formula("Diagnosis ~ Pregnancies + Glucose + BloodPressure + BMI + Pedigree + Age")

# Fit the logistic regression model
LRM1 <- glm(formula, data = balanced_data, family = "binomial")

summary(LRM1)
```

We select vraible by removing the Insulin and skinthcikness since it has exceptionaly high number of 0

# J)  Model Report
```{r}
# Select the desired columns including the "Diagnosis" column
selected_data <- balanced_data[, c("Pregnancies", "Glucose", "BloodPressure", "BMI", "Pedigree", "Age", "Diagnosis")]

# Predict using the model
predictions <- predict(LRM1, newdata = selected_data, type = "response")

# Convert predicted probabilities to class labels (0 or 1)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# Convert predicted_classes and standdata$Diagnosis to factors with the same levels
predicted_classes <- factor(predicted_classes, levels = c(0, 1))
selected_data$Diagnosis <- factor(selected_data$Diagnosis, levels = c(0, 1))

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predicted_classes, balanced_data$Diagnosis)

# Calculate accuracy
accuracy <- confusion_matrix$overall['Accuracy']

# Print the accuracy
print(accuracy)
print(confusion_matrix)

```
The accuracy is 76.9% which mean from the data trained it only able to correctly predict 76.9% of it

# K) classification report (precision, recall, F1 score, and support)

```{r}
# Calculate precision
precision <- confusion_matrix$byClass["Pos Pred Value"]

# Calculate recall
recall <- confusion_matrix$byClass["Sensitivity"]

# Calculate F1 score
f1_score <- 2 * (precision * recall) / (precision + recall)

# Calculate support
support <- confusion_matrix$table[2,2]  # True positive count

# Print the results
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", f1_score, "\n")
cat("Support:", support, "\n")
```


the precision value of 0.75   means that out of all the instances predicted as positive, around 75% were correctly classified as positive.

The recall value of 0.786  indicates that the model correctly identified around 78.6% of the actual positive instances.

The F1 score value of 0.7675781  suggests a good balance between precision and recall for the model's performance.

Support: Support refers to the number of actual instances belonging to the positive class in the dataset. In this case, the support value of 369  indicates that there are 369  instances classified as positive in the dataset.


# L) accuracy score of your model **** not finished since i dont split the dataset

```{r}
# Make predictions on test data
predictions_LRM1 <- predict(LRM1, newdata = balanced_data, type = "response")

# Calculate accuracy score for LRM1
accuracy_LRM1 <- sum(predictions_LRM1 == balanced_data$Diagnosis) / length(predictions_LRM1)
print(paste("Accuracy Score (LRM1):", accuracy_LRM1))
```
# M) Model LRM2 - Full model

```{r}
# Load the required library
library(glmnet)

# Create the formula for logistic regression
formula <- as.formula("Diagnosis ~ .")

# Fit the logistic regression model
LRM2 <- glm(formula, data = balanced_data, family = "binomial")

summary(LRM2)


```

```{r}
# Predict using the model
predicted_classes <- ifelse(predict(LRM2, newdata = balanced_data, type = "response") > 0.5, 1, 0)


# Create a confusion matrix
confusion_matrix <- table(predicted_classes, balanced_data$Diagnosis)

# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

# Print the confusion matrix and accuracy
print(confusion_matrix)
print(paste("Accuracy:", accuracy))

```
The overall has slightly higher accuracy. it shows 76.3%

## N) Compare
```{r}
# Perform likelihood ratio test
lr_test <- anova(LRM1, LRM2, test = "LRT")

# Extract the test statistic, degrees of freedom, and p-value
test_statistic <- lr_test$Deviance[2]
degrees_of_freedom <- lr_test$Df[2]
p_value <- 1 - pchisq(test_statistic, degrees_of_freedom)

# Print the test results
print(paste("Test Statistic:", test_statistic))
print(paste("Degrees of Freedom:", degrees_of_freedom))
print(paste("p-value:", p_value))
```
The obtained test statistic is 4.21, indicating that LRM2 provides a better fit than LRM1.
The degrees of freedom are 2, representing the difference in the number of parameters between the two models.
The p-value is 0.12, which is greater than the commonly used significance level of 0.05. This suggests that there is not enough evidence to reject the null hypothesis, indicating that the improvement provided by LRM2 is not significantly diffrent from LRM1.

# O) Examine the coefficients - direction and significance of the relationship between the predictor variables and the outcome variable

```{r}
# View the coefficient estimates
coef_summary <- summary(LRM1)$coefficients
print(coef_summary)

# Access the coefficient values
coefficients <- coef_summary[, "Estimate"]

# Access the p-values
p_values <- coef_summary[, "Pr(>|z|)"]

# Loop through the coefficients and interpret the results
for (i in 1:length(coefficients)) {
  coefficient <- coefficients[i]
  p_value <- p_values[i]
  
  if (p_value < 0.05) {
    significance <- "significant"
  } else {
    significance <- "not significant"
  }
  
  if (coefficient > 0) {
    direction <- "positive"
  } else if (coefficient < 0) {
    direction <- "negative"
  } else {
    direction <- "no"
  }
  
  predictor <- names(coefficients)[i + 1]  # Skip the intercept term
  
  cat(paste("The coefficient for", predictor, "is", coefficient,
            "indicating a", direction, "relationship, and is", significance, "\n"))
}
```

# P) hypothesis tests that your model is significantly better.
Bernouli Test
```{r}
# Set the desired significance level
significance_level <- 0.05

# Calculate the number of hypothesis tests (in this case, 1)
num_tests <- 1



# Perform likelihood ratio test
lr_test <- anova(LRM1, LRM2, test = "LRT")

# Extract the test statistic, degrees of freedom, and p-value
test_statistic <- lr_test$Deviance[2]
degrees_of_freedom <- lr_test$Df[2]
p_value <- 1 - pchisq(test_statistic, degrees_of_freedom)


significance_level <- 0.05
num_tests <- 4
# Adjust the significance level using the Bonferroni correction
adjusted_significance_level <- significance_level / num_tests

# Compare the p-value with the adjusted significance level
if (p_value < adjusted_significance_level) {
  print("Reject the null hypothesis - LRM2 is significantly better than LRM1")
} else {
  print("Fail to reject the null hypothesis - LRM2 is not significantly better than LRM1")
}
```

# q) Bonferroni correction
```{r}
# Original significance level
alpha <- 0.05

# Number of hypothesis tests
num_tests <- 1

# Bonferroni-corrected significance level
adjusted_alpha <- alpha / num_tests

# Perform hypothesis test using adjusted significance level
if (p_value < adjusted_alpha) {
  interpretation <- "Reject the null hypothesis. Model 1 is significantly better than Model 0."
} else {
  interpretation <- "Fail to reject the null hypothesis. Model 1 is not significantly better than Model 0."
}

# Print the adjusted significance level and interpretation
cat("Adjusted Significance Level:", adjusted_alpha, "\n")
cat("Interpretation:", interpretation, "\n")
```

# r) suggestions for further improving the accuracy of your chosen model

The following are some suggestions for enhancing the chosen model's accuracy further:

1. Engineering of Features: Investigate additional methods of feature engineering to construct relevant new features from existing ones. Combining features, generating interaction terms, or extracting relevant data are all examples of this.

2. Selection of Features: Utilize techniques for feature selection to identify the model's most essential features. This can help focus on the most relevant predictors while reducing noise. Recursive Feature Elimination (RFE), Lasso regression, and feature importance from tree-based models are all options.

3. Choosing a Model: Try different things with various sorts of models or troupe strategies to check whether they can all the more likely catch the fundamental examples in the information. Try random forests, gradient boosting machines, or support vector machines as an algorithm.

4. Tuning the Hyperparameters: Optimize the model's performance by fine-tuning its hyperparameters. To find the optimal hyperparameter combination, methods like grid search, random search, or Bayesian optimization can be used.

5. Cross-Validation: To avoid overfitting and obtain a more robust estimate of the model's performance, employ cross-validation methods like k-fold cross-validation. This aids in determining the model's capacity for generalization.

6. Taking care of Class Unevenness: On the off chance that the dataset has class unevenness, where one class is fundamentally more predominant than the other, consider utilizing strategies, for example, oversampling the minority class (e.g., Destroyed) or undersampling the greater part class to adjust the dataset and work on model execution.

7. More Data Collection: To increase the dataset's size, gather additional data whenever possible. The model can learn more effectively and better generalize to unobserved examples with more data.

8. Explore Blunders: Examine the model's mistakes to learn more about the kinds of situations it faces. This can direct further enhancements, like gathering extra highlights or tending to explicit examples in the information.

9. Regularization: To avoid overfitting and boost model performance, use regularization methods like L1 and L2 regularization.

10. Skills in the Field: Include domain expertise in the modeling procedure. Think about consulting domain experts to learn more about the problem's particular characteristics and incorporate their insights into feature engineering or model design.

It is essential to keep in mind that the problem and dataset at hand may have different effects on these suggestions' efficacy. It is prescribed to explore different avenues regarding various methodologies and cautiously assess their effect on the model's presentation.


# Improving Model by reducing TYPE-II ERROR



