---
title: "gestational_diabetes"
output: pdf_document
date: "2023-05-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
install.packages("visdat")
```

#Library
```{r}
library(ggplot2)
library(dplyr)
library(gridExtra)
library(naniar)
library(visdat)
```


#Data
```{r}
data <- read.csv(url("https://raw.githubusercontent.com/Marchosh/gestational_diabetes/master/patients.csv?token=GHSAT0AAAAAACC7K4VFNJCC5HTZ23CRGRBAZDMELKA"))
head(data)
```


```{r}
summary(data)

```
Pregnancies:
The range of the number of pregnancies is from 0 to 17, with a median of 3 and a mean of approximately 3.845.
The majority of individuals (75%) had 6 or fewer pregnancies (as indicated by the third quartile).

Glucose:
The minimum glucose level observed is 0, which seems unusual and may indicate missing or invalid data.
The range of glucose levels is from 0 to 199, with a median of 117 and a mean of approximately 120.9.
The majority of glucose levels (75%) fall below 140.2 (as indicated by the third quartile).

BloodPressure:
The range of diastolic blood pressure values is from 0 to 122, with a median of 72 and a mean of approximately 69.11.
The majority of blood pressure readings (75%) are below 80 (as indicated by the third quartile).

SkinThickness:
The range of triceps skinfold thickness values is from 0 to 99, with a median of 23 and a mean of approximately 20.54.
The majority of skinfold thickness measurements (75%) are below 32 (as indicated by the third quartile).

Insulin:
The range of serum insulin levels is from 0 to 846, with a median of 30.5 and a mean of approximately 79.8.
The majority of insulin levels (75%) are below 127.2 (as indicated by the third quartile).

BMI:
The range of body mass index (BMI) values is from 0 to 67.1, with a median of 32 and a mean of approximately 31.99.
The majority of individuals (75%) have a BMI below 36.6 (as indicated by the third quartile).

Pedigree:
The range of diabetes pedigree function values is from 0.0780 to 2.4200, with a median of 0.3725 and a mean of approximately 0.4719.
The majority of pedigree function values (75%) fall below 0.6262 (as indicated by the third quartile).

Age:
The range of ages in the dataset is from 21 to 81, with a median of 29 and a mean of approximately 33.24.
The majority of individuals (75%) are below 41 years old (as indicated by the third quartile).

Diagnosis:
The diagnosis variable is binary, with 0 representing a negative diagnosis (no diabetes) and 1 representing a positive diagnosis (diabetes).
Approximately 34.9% of the cases in the dataset are diagnosed with diabetes, as indicated by the mean.

# EDA
```{r}
# Bar plot of Diagnosis
ggplot(data, aes(x = factor(Diagnosis))) +
  geom_bar(fill = "lightblue") +
  labs(x = "Diagnosis", y = "Count") +
  ggtitle("Distribution of Diagnosis")

```

```{r}
# Exclude non-numeric variables
vars <- names(data)[sapply(data, is.numeric)]

# Loop through the variables and create scatter plots
for (var in vars) {
  plot <- ggplot(data, aes(x = .data[[var]], y = Diagnosis, color = as.factor(Diagnosis))) +
    geom_point() +
    labs(x = var, y = "Diagnosis", color = "Diagnosis") +
    ggtitle(paste("Scatter Plot of", var, "against Diagnosis")) +
    theme(legend.position = "bottom") +
    scale_color_brewer(palette = "Set1")
  
  print(plot)
}

```
```{r}
# Exclude non-numeric variables and the "Diagnosis" column
vars <- names(data)[!sapply(data, is.factor)]
vars <- vars[vars != "Diagnosis"]

# Create a list to store the plots
plots <- list()

# Loop through the variables and create box plots
for (var in vars) {
  plot <- ggplot(data, aes(x = as.factor(Diagnosis), y = .data[[var]], fill = as.factor(Diagnosis))) +
    geom_boxplot() +
    labs(x = "Diagnosis", y = var, fill = "Diagnosis") +
    ggtitle(paste("Box Plot of", var, "by Diagnosis")) +
    theme(legend.position = "bottom") +
    scale_fill_brewer(palette = "Set1")
  
  plots[[var]] <- plot
}

# Display the plots
for (plot in plots) {
  print(plot)
}

```
## histogram
```{r}
# Create histograms for each variable
numeric_vars <- names(data)[sapply(data, is.numeric)]
for (var in numeric_vars) {
  p <- ggplot(data, aes(x = .data[[var]])) +
    geom_histogram(binwidth = 5, fill = "lightblue", color = "black") +
    labs(x = var, y = "Frequency", title = paste("Histogram of", var)) +
    theme_minimal()
  
  print(p)
}
```
Pregnancies:

Glucose:
there is zero value which missing value mostlikely

Blood Preasue:
there si possibility of missing value in the blood preaseure since 0 is present

Skin thickness:
missing value 0

Insulin:
hug spike in 0 must be investivigated

BMI:
0 porbabluy missing value

Pedigree:
all equal, need to be investivigated

AGE:
seem some outlier with women over 60

Diagnosis:
Binary

## violin plot
```{r}
# Create violin plots for each variable
numeric_vars <- names(data)[sapply(data, is.numeric)]
for (var in numeric_vars) {
  p <- ggplot(data, aes(x = as.factor(Diagnosis), y = .data[[var]], fill = as.factor(Diagnosis))) +
    geom_violin(trim = FALSE, scale = "width") +
    labs(x = "Diagnosis", y = var, title = paste("Violin Plot of", var, "by Diagnosis")) +
    theme_minimal()
  
  print(p)
}

```

```{r}
# Create density plots for each variable
numeric_vars <- names(data)[sapply(data, is.numeric)]
for (var in numeric_vars) {
  p <- ggplot(data, aes(x = .data[[var]], fill = as.factor(Diagnosis))) +
    geom_density(alpha = 0.5) +
    labs(x = var, y = "Density", title = paste("Density Plot of", var, "by Diagnosis")) +
    theme_minimal()
  print(p)
}

```


# missing value

```{r}
# Calculate the number of missing values for each variable
missing_counts <- data %>%
  summarise_all(~sum(is.na(.) | . == 0))

# Convert missing_counts to long format for plotting
missing_counts_long <- tidyr::pivot_longer(missing_counts, everything(), names_to = "Variable", values_to = "Missing_Count")

# Create the bar plot
p <- ggplot(missing_counts_long, aes(x = Variable, y = Missing_Count, fill = Variable)) +
  geom_bar(stat = "identity") +
  labs(x = "Variable", y = "Missing Count", title = "Number of Missing Values by Variable") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p)

```

```{r}
# Replace 0 with NA for missing values
data[data == 0] <- NA

# Plot the modified missing values heatmap
vis_miss(data)
```

Diagnosis 0 is category so is not mising value,
THe insulin 0 is need to be investivigate wheter is it true 0 or missing value beacsue 49% of them is 0
Same goes fro skin thcikness, wheter is it true 0 or missing value becaseu contain 30%
For the preganancies 0 it could indicate they never pregnant and not missing value
But the other (Glucose, BloodPreassure, and BMi) variable 0 is mostlikely missing value.

We can see straight long line continous from one oclumn to other it showed that,
when 1 of the column missing the other column are also likely to be missing



# Outlier
```{r}
# Identify the numeric independent variables
numeric_vars <- names(data)[sapply(data, is.numeric)]

# Remove rows with missing values
data_complete <- na.omit(data)

# Create a box plot for each independent variable
for (var in numeric_vars) {
  p <- ggplot(data_complete, aes(x = Diagnosis, y = data_complete[[var]])) +
    geom_boxplot(fill = "lightblue", color = "black") +
    labs(x = "Diagnosis", y = var, title = paste("Box Plot of", var, "by Diagnosis")) +
    theme_minimal()
  print(p)
}
```


```{r}
# Identify the numeric independent variables
numeric_vars <- names(data)[sapply(data, is.numeric)]

# Find outliers using the IQR rule
outliers <- lapply(data[numeric_vars], function(var) {
  Q1 <- quantile(var, 0.25, na.rm = TRUE)
  Q3 <- quantile(var, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  outliers <- var[var < lower_bound | var > upper_bound]
  return(outliers)
})

# Print the outliers for each variable
for (i in 1:length(numeric_vars)) {
  var <- numeric_vars[i]
  if (length(outliers[[i]]) > 0) {
    cat("Outliers for", var, ":\n")
    print(outliers[[i]])
    cat("\n")
  } else {
    cat("No outliers found for", var, "\n\n")
  }
}
```


--try this method---
Robust Statistical Methods: Robust statistical methods, such as robust regression or robust estimators, can handle outliers more effectively by downweighting their influence in the analysis. These methods are less sensitive to extreme values and can provide more reliable estimates.



Winsorization: Winsorization replaces extreme values with values closer to the rest of the data. Instead of removing the outliers completely, you can replace them with a trimmed or truncated value at a certain percentile. This approach retains the overall distribution shape while reducing the impact of outliers

```{r}
# Apply Winsorization to replace outliers in selected variables
selected_vars <- c("Pregnancies", "BloodPressure", "BMI", "Insulin", "SkinThickness", "Pedigree")

winsorize <- function(x, trim = 0.05) {
  q <- quantile(x, probs = c(trim, 1 - trim), na.rm = TRUE)
  x[x < q[1]] <- q[1]
  x[x > q[2]] <- q[2]
  x
}

for (var in selected_vars) {
  cleandata[, var] <- winsorize(cleandata[, var])
}
```


### Box plot After replacement

```{r}
# Create box plot for each variable
for (var in names(cleandata)) {
  p <- ggplot(cleandata, aes(x = 1, y = cleandata[[var]])) +
    geom_boxplot(fill = "lightblue", color = "black") +
    labs(x = "", y = var, title = paste("Box Plot of", var)) +
    theme_minimal() +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
  print(p)
}

```



# Select Varaible
```{r}
# Calculate the correlation matrix
cor_matrix <- cor(cleandata)

# Create a correlogram using a heatmap

corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7, addCoef.col = "black")


```

Glucose has the strongest correlation with the Diagnosis

There is also posisbilities of multicoloniarity between Preagnancies and Age
and between Insulin and skinthcikness

Skinthickness and Insulin is removed since it has great umber of 0


# Standarization
Standardizing features to a Gaussian distribution can be a good idea for several reasons:

Equalize the scales: Standardizing the features ensures that they are on a comparable scale. This is important when working with algorithms that are sensitive to the scale of the variables. Standardization prevents features with larger scales from dominating the algorithm and helps to ensure fair comparisons between different features.

Facilitate model convergence: Many machine learning algorithms, such as linear regression and neural networks, rely on optimization techniques that assume the input features are normally distributed or have a similar scale. Standardizing the features can improve the convergence of these algorithms and help them find the optimal solution more efficiently.

Interpretability and comparability: When features are standardized, their values are transformed to a common scale, making them more interpretable and comparable. The standardized values represent the number of standard deviations the original values are from the mean. This allows for easier interpretation and understanding of the relative importance and impact of each feature on the model.

Reduce the influence of outliers: Standardization can help mitigate the impact of outliers on the model. By transforming the features to a Gaussian distribution, extreme values (outliers) are scaled to a smaller range and have less influence on the model's behavior. This can lead to more robust and stable model performance.

Improve feature importance estimation: Standardization helps to provide a fair estimation of feature importance or feature contribution in models that use regularization techniques or rely on feature weights. It ensures that the importance or weights are not biased by the original scale of the features.

Overall, standardizing features to a Gaussian distribution is a common practice in data preprocessing to improve the performance, stability, and interpretability of machine learning models. It helps to address issues related to feature scales, convergence, interpretability, outliers, and fair comparison between features.


```{r}
standdata <- cleandata

# Select the independent variables
independent_vars <- names(cleandata)[names(cleandata) != "Diagnosis"]

# Standardize the independent variables using Gaussian distribution
stand_data <- as.data.frame(scale(cleandata[, independent_vars]))

standdata[, independent_vars] <- stand_data
head(standdata, 10)
```
```{r}
summary(standdata)

```

## Normalize Data (Diagnosis)
```{r}
# Convert "Diagnosis" column to factor
standdata$Diagnosis <- as.factor(standdata$Diagnosis)

# Check class distribution
table(standdata$Diagnosis)

# Get the indices of the minority and majority class
minority_indices <- which(standdata$Diagnosis == 1)
majority_indices <- which(standdata$Diagnosis == 0)

# Oversample the minority class
oversampled_minority_indices <- sample(minority_indices, 500, replace = TRUE)

# Undersample the majority class
undersampled_majority_indices <- sample(majority_indices, 500)

# Combine the minority and balanced majority indices
balanced_indices <- c(oversampled_minority_indices, undersampled_majority_indices)

# Create the balanced dataset
balanced_data <- standdata[balanced_indices, ]

# Check the class distribution of the balanced dataset
table(balanced_data$Diagnosis)
```

```{r}
balanced_data
standdata
```

# Model

```{r}
# Load the required library
library(glmnet)

# Create the formula for logistic regression
formula <- as.formula("Diagnosis ~ Pregnancies + Glucose + BloodPressure + BMI + Pedigree + Age")

# Fit the logistic regression model
LRM1 <- glm(formula, data = balanced_data, family = "binomial")

summary(LRM1)
```


```{r}
# Select the desired columns including the "Diagnosis" column
selected_data <- balanced_data[, c("Pregnancies", "Glucose", "BloodPressure", "BMI", "Pedigree", "Age", "Diagnosis")]

# Predict using the model
predictions <- predict(LRM1, newdata = selected_data, type = "response")

# Convert predicted probabilities to class labels (0 or 1)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# Convert predicted_classes and standdata$Diagnosis to factors with the same levels
predicted_classes <- factor(predicted_classes, levels = c(0, 1))
selected_data$Diagnosis <- factor(selected_data$Diagnosis, levels = c(0, 1))

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predicted_classes, LMR1DATA$Diagnosis)

# Calculate accuracy
accuracy <- confusion_matrix$overall['Accuracy']

# Print the accuracy
print(accuracy)
print(confusion_matrix)
```
















